{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcbmu5hMoa-7"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "This notebook performs text preprocessing on the data present in the file: `quora-question-pairs/train.csv`\n",
        "\n",
        "We the following approach for text preprocessing:\n",
        "1. Tokenizing\n",
        "2. Lowercasing\n",
        "3. Noise Removal\n",
        "4. Stop-word Removal\n",
        "5. Lemmatization\n",
        "6. Normalisation\n",
        "\n",
        "### To do:\n",
        "\n",
        "- [ ] Provide description for all processes and reason to perform them\n",
        "- [ ] Describe Directory Structure\n",
        "- [x] Load Dataset into pandas\n",
        "- [x] Perform Tokenizing\n",
        "- [x] Perform Lowercasing\n",
        "- [x] Perform Noise Removal\n",
        "- [x] Remove Stop-words\n",
        "- [x] Perform Lemmatization\n",
        "- [ ] Perform Normalisation/Spelling correction\n",
        "- [ ] Prepare Datasets:\n",
        "    - [ ] With Lemmatization and without Normalisation\n",
        "    - [ ] Without Lemmatization and with Normalisation\n",
        "    - [ ] Without Lemmatization and without Normalisation\n",
        "    - [ ] With Lemmatization and with Normalisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpa8Ugfdoa_P"
      },
      "source": [
        "### Import Statements and File Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCfxb1kYoa_R",
        "outputId": "955753ea-ebd5-460a-9184-190e3e697dbb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/Quora-Data/\"\n",
        "train_csv = data_dir + 'pre-processing/train.csv'\n",
        "test_csv = data_dir + 'pre-processing/test.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZLAB3C99YnA",
        "outputId": "ea9f407b-beba-4ea8-d882-f232b5438520"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UduZbowouXq",
        "outputId": "115428ad-c984-4f17-b694-71a8ebad07b6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLh_xXR5oa_U"
      },
      "source": [
        "### [Step 0] Loading Dataset into Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "3fJiEbJyoa_V",
        "outputId": "6bad4381-133f-4612-fcc9-f43a682d20ee"
      },
      "source": [
        "data_train = pd.read_csv(train_csv)\n",
        "data_test = pd.read_csv(test_csv)\n",
        "display(data_train)\n",
        "display(data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>394437</td>\n",
              "      <td>434361</td>\n",
              "      <td>527326</td>\n",
              "      <td>How do I install APK files on my Windows Phone?</td>\n",
              "      <td>How can I backup a (.xap, / . APPX) file insta...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>373988</td>\n",
              "      <td>8023</td>\n",
              "      <td>10567</td>\n",
              "      <td>What were the major effects of the cambodia ea...</td>\n",
              "      <td>What were the major effects of the cambodia ea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>183101</td>\n",
              "      <td>280083</td>\n",
              "      <td>280084</td>\n",
              "      <td>How is Stack Exchange better than Quora?</td>\n",
              "      <td>Is Stack Exchange better than Quora? Why or wh...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43553</td>\n",
              "      <td>78324</td>\n",
              "      <td>78325</td>\n",
              "      <td>How to prevent from pimples to break out insid...</td>\n",
              "      <td>How can I avoid getting pimples inside my nose?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>213919</td>\n",
              "      <td>319381</td>\n",
              "      <td>46044</td>\n",
              "      <td>What are some good books and online courses to...</td>\n",
              "      <td>What is a good online course on probability an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323427</th>\n",
              "      <td>346872</td>\n",
              "      <td>475259</td>\n",
              "      <td>475260</td>\n",
              "      <td>What is the benefit of using const for declari...</td>\n",
              "      <td>What is the benefit of using enum to declare a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323428</th>\n",
              "      <td>143678</td>\n",
              "      <td>14804</td>\n",
              "      <td>40458</td>\n",
              "      <td>How can I get a complete list of all old Gmail...</td>\n",
              "      <td>How do I find my own gmail accounts list?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323429</th>\n",
              "      <td>128137</td>\n",
              "      <td>14317</td>\n",
              "      <td>34001</td>\n",
              "      <td>Where can I found modern colours and textures ...</td>\n",
              "      <td>Where can I get wide range of floor tile, wall...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323430</th>\n",
              "      <td>323891</td>\n",
              "      <td>449906</td>\n",
              "      <td>449907</td>\n",
              "      <td>Support@ 1877#778#89.69 ACER Technical support...</td>\n",
              "      <td>@Support@ 1877#778#89.69 COMPAQ Technical supp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323431</th>\n",
              "      <td>228034</td>\n",
              "      <td>337010</td>\n",
              "      <td>337011</td>\n",
              "      <td>How are lollipops and gummy bears manufactured?</td>\n",
              "      <td>How do lollipops and gummy bears differ?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>323432 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id    qid1  ...                                          question2  Y\n",
              "0       394437  434361  ...  How can I backup a (.xap, / . APPX) file insta...  0\n",
              "1       373988    8023  ...  What were the major effects of the cambodia ea...  1\n",
              "2       183101  280083  ...  Is Stack Exchange better than Quora? Why or wh...  1\n",
              "3        43553   78324  ...    How can I avoid getting pimples inside my nose?  1\n",
              "4       213919  319381  ...  What is a good online course on probability an...  0\n",
              "...        ...     ...  ...                                                ... ..\n",
              "323427  346872  475259  ...  What is the benefit of using enum to declare a...  0\n",
              "323428  143678   14804  ...          How do I find my own gmail accounts list?  1\n",
              "323429  128137   14317  ...  Where can I get wide range of floor tile, wall...  1\n",
              "323430  323891  449906  ...  @Support@ 1877#778#89.69 COMPAQ Technical supp...  0\n",
              "323431  228034  337010  ...           How do lollipops and gummy bears differ?  0\n",
              "\n",
              "[323432 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>350255</td>\n",
              "      <td>478982</td>\n",
              "      <td>478983</td>\n",
              "      <td>Studying: I have made easy handwritten notes t...</td>\n",
              "      <td>From where can I download different institutes...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49376</td>\n",
              "      <td>87885</td>\n",
              "      <td>87886</td>\n",
              "      <td>Why didn't Melisandre make more shadow babies?</td>\n",
              "      <td>Why wouldn't Stannis use Melisandre's shadow b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>276580</td>\n",
              "      <td>395562</td>\n",
              "      <td>395563</td>\n",
              "      <td>Why do we fix one gear in epicyclic gear train...</td>\n",
              "      <td>Does Antarctica have any geopolitical importance?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13757</td>\n",
              "      <td>26386</td>\n",
              "      <td>26387</td>\n",
              "      <td>I sent text to my friends on WhatsApp and ther...</td>\n",
              "      <td>Lately, I sent a WhatsApp message to a friend ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>307764</td>\n",
              "      <td>431504</td>\n",
              "      <td>431505</td>\n",
              "      <td>Can machine learning be used for Borderline PD...</td>\n",
              "      <td>How does Borderline PD affect writing ability?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80853</th>\n",
              "      <td>159203</td>\n",
              "      <td>40624</td>\n",
              "      <td>27380</td>\n",
              "      <td>What are the safety precautions on handling sh...</td>\n",
              "      <td>What are the safety precautions on handling sh...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80854</th>\n",
              "      <td>200725</td>\n",
              "      <td>37131</td>\n",
              "      <td>302569</td>\n",
              "      <td>How can you find the molar mass of deuterium?</td>\n",
              "      <td>How do you find the molar mass of ionic compou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80855</th>\n",
              "      <td>179551</td>\n",
              "      <td>275483</td>\n",
              "      <td>275484</td>\n",
              "      <td>Which key witnesses supported the death penalt...</td>\n",
              "      <td>Which key witnesses did not support the death ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80856</th>\n",
              "      <td>115590</td>\n",
              "      <td>188458</td>\n",
              "      <td>188459</td>\n",
              "      <td>Should dams be built or not?</td>\n",
              "      <td>How are dams built?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80857</th>\n",
              "      <td>253135</td>\n",
              "      <td>367605</td>\n",
              "      <td>367606</td>\n",
              "      <td>Why the price varies for a 2 BHK apartment in ...</td>\n",
              "      <td>What are my chances of getting an offer from G...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80858 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id    qid1  ...                                          question2  Y\n",
              "0      350255  478982  ...  From where can I download different institutes...  0\n",
              "1       49376   87885  ...  Why wouldn't Stannis use Melisandre's shadow b...  0\n",
              "2      276580  395562  ...  Does Antarctica have any geopolitical importance?  0\n",
              "3       13757   26386  ...  Lately, I sent a WhatsApp message to a friend ...  0\n",
              "4      307764  431504  ...     How does Borderline PD affect writing ability?  0\n",
              "...       ...     ...  ...                                                ... ..\n",
              "80853  159203   40624  ...  What are the safety precautions on handling sh...  1\n",
              "80854  200725   37131  ...  How do you find the molar mass of ionic compou...  0\n",
              "80855  179551  275483  ...  Which key witnesses did not support the death ...  0\n",
              "80856  115590  188458  ...                                How are dams built?  0\n",
              "80857  253135  367605  ...  What are my chances of getting an offer from G...  0\n",
              "\n",
              "[80858 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ReBBaizoa_a"
      },
      "source": [
        "# data.rename(columns = {'question1':'q1_orig', 'question2':'q2_orig'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRQvJ2V5oa_c",
        "outputId": "4690b63e-84aa-4694-b334-a6937676f367"
      },
      "source": [
        "data_train['Y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    203899\n",
              "1    119533\n",
              "Name: Y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAm8MF3vsAvM",
        "outputId": "50f91690-70a5-43ba-bb8b-2060e525df19"
      },
      "source": [
        "data_test['Y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    51128\n",
              "1    29730\n",
              "Name: Y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4eFyO7oa_d"
      },
      "source": [
        "As can be seen there is a class imbalance, of roughly 63:36"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn9XIfVroa_f"
      },
      "source": [
        "We also observe that we have been given question ids, we try and see if there are any repeated question ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPEfG6_Ioa_h"
      },
      "source": [
        "### [Step 1] Lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyqB03aNoa_l"
      },
      "source": [
        "data_train['question1'] = data_train['question1'].str.lower()\n",
        "data_train['question2'] = data_train['question2'].str.lower()\n",
        "\n",
        "data_test['question1'] = data_test['question1'].str.lower()\n",
        "data_test['question2'] = data_test['question2'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U4-0IXmoa_l"
      },
      "source": [
        "### [Step 2] Tokenizing\n",
        "\n",
        "White Space tokenizing with NLTK\n",
        "\n",
        "### [Step 3]  Noise Removal\n",
        "\n",
        "Not sure what to do for this step.. I think the data we have is clean because theire aren't any HTML tags or emojis etc. However, there are symbols such as : ^ * { } ( ) \\[ \\] \\ & シ し \n",
        "I am not sure if we want to eliminate them or keep them? Some of these symbols are being used for math equations too. For example: [math]y=\\frac{4x^2 - 36x}{ x-9}[/math]\n",
        "\n",
        "### [Step 4] Removing Stop-words\n",
        "\n",
        "Using NLTK stop-words\n",
        "\n",
        "### [Step 5] Lemmatization\n",
        "\n",
        "Using NLTK WordNet\n",
        "\n",
        "### [Step 6] Normalisation: Spelling correction \n",
        "\n",
        "Perform before lemmatization using: https://towardsdatascience.com/text-normalization-7ecc8e084e31\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwlx7ncGoa_n"
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tokenize(question_body):\n",
        "    tokens = word_tokenize(question_body)\n",
        "    return tokens\n",
        "\n",
        "def remove_symbols(tokens):\n",
        "    review_text = \" \".join(tokens)\n",
        "    # review_text = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`]\", \" \", review_text)\n",
        "    # review_text = re.sub(r\"\\'s\", \" 's \", review_text)\n",
        "    # review_text = re.sub(r\"\\'ve\", \" 've \", review_text)\n",
        "    # review_text = re.sub(r\"n\\'t\", \" 't \", review_text)\n",
        "    # review_text = re.sub(r\"\\'re\", \" 're \", review_text)\n",
        "    # review_text = re.sub(r\"\\'d\", \" 'd \", review_text)\n",
        "    # review_text = re.sub(r\"\\'ll\", \" 'll \", review_text)\n",
        "    # review_text = re.sub(r\",\", \" \", review_text)\n",
        "    # review_text = re.sub(r\"\\.\", \" \", review_text)\n",
        "    # review_text = re.sub(r\"!\", \" \", review_text)\n",
        "    # review_text = re.sub(r\"\\(\", \" ( \", review_text)\n",
        "    # review_text = re.sub(r\"\\)\", \" ) \", review_text)\n",
        "    # review_text = re.sub(r\"\\?\", \" \", review_text)\n",
        "    # review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n",
        "\n",
        "    text = review_text\n",
        "\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    words = review_text.split()\n",
        "    return(words)\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    keywords = [word for word in tokens if not word in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def spelling_correction(tokens):\n",
        "    #implement spelling correction if better performance is reqd\n",
        "    pass\n",
        "\n",
        "def lemmatize_text(tokens):\n",
        "    for word in range(0,len(tokens)):\n",
        "        tokens[word] = lemmatizer.lemmatize(tokens[word])\n",
        "    return tokens\n",
        "\n",
        "def concatenate_tokens(tokens):\n",
        "    joined = str(\" \".join(tokens))\n",
        "    return(joined)\n",
        "\n",
        "def pre_processing_with_lemmatization(question_body):\n",
        "    tokens = tokenize(question_body)\n",
        "    keywords = remove_stop_words(tokens)\n",
        "    keywords = remove_symbols(keywords)\n",
        "    lemmatized_tokens = lemmatize_text(keywords)\n",
        "    joined = concatenate_tokens(lemmatized_tokens)\n",
        "    return(joined)\n",
        "\n",
        "def pre_processing_without_lemmatization_and_spell_correct(question_body):\n",
        "    tokens = tokenize(question_body)\n",
        "    keywords = remove_stop_words(tokens)\n",
        "    keywords = remove_symbols(keywords)\n",
        "    joined = concatenate_tokens(keywords)\n",
        "    return(joined)\n",
        "\n",
        "def pre_processing_with_spell_correct(question_body):\n",
        "    tokens = tokenize(question_body)\n",
        "    keywords = remove_stop_words(tokens)\n",
        "    keywords = remove_symbols(keywords)\n",
        "    spell_checked = spelling_correction(keywords)\n",
        "    joined = concatenate_tokens(keywords)\n",
        "    return(joined)\n",
        "\n",
        "def pre_processing_with_lemmatization_and_spell_correct(question_body):\n",
        "    tokens = tokenize(question_body)\n",
        "    keywords = remove_stop_words(tokens)\n",
        "    keywords = remove_symbols(keywords)\n",
        "    spell_checked = spelling_correction(keywords)\n",
        "    lemmatized_tokens = lemmatize_text(spell_checked)\n",
        "    joined = concatenate_tokens(keywords)\n",
        "    return(joined)\n",
        "\n",
        "data_train['question1']=data_train['question1'].apply(str)\n",
        "data_train['question2']=data_train['question2'].apply(str)\n",
        "data_test['question1']=data_test['question1'].apply(str)\n",
        "data_test['question2']=data_test['question2'].apply(str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qtXZiwCoa_o",
        "outputId": "90cac44a-38be-40fe-e794-5b63c3f7e887"
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"aren't\", \"you'd\", 'm', 'been', \"you'll\", 'here', 'your', 'ain', \"you're\", 't', 'wasn', 'but', 'me', 'they', 'all', 'will', 'with', 'once', \"shouldn't\", 'to', 'its', 'can', 'aren', 'an', 'her', 'did', 'too', 'is', \"that'll\", \"she's\", 'same', 'he', \"couldn't\", 'needn', 'ours', \"don't\", 'whom', 'nor', 'o', 'below', 'being', 'now', 'for', 'again', 'each', 'be', 'during', 'isn', 'shan', 'both', 'until', 'hers', 'was', 'not', \"it's\", 'about', 'mightn', 'between', 'doesn', 'our', 'have', 'a', 'itself', 'that', 'myself', \"weren't\", 'because', 'such', 'any', 'there', 'mustn', 'so', 'having', 'no', 's', 'himself', 'd', 'didn', 'down', 'ma', 'theirs', 'does', 'own', 'this', 'what', 'as', 'yours', 'these', \"mustn't\", \"should've\", 'shouldn', 'out', \"didn't\", 'my', 'themselves', 'further', 'than', 'y', 'on', \"wouldn't\", 'when', 'few', 'should', 'll', 'over', 'above', \"haven't\", 'just', 'she', 'had', \"you've\", 've', 'them', 're', 'how', 'wouldn', 'him', 'i', 'before', 'we', 'in', 'some', 'doing', 'under', 'why', 'haven', 'up', \"needn't\", \"doesn't\", 'only', 'and', 'while', 'hadn', 'which', 'if', 'yourselves', 'into', 'yourself', 'more', 'their', 'by', 'don', 'it', \"hasn't\", \"isn't\", 'his', 'has', 'through', 'other', 'herself', \"won't\", \"hadn't\", 'most', 'are', 'the', 'then', 'couldn', \"shan't\", 'weren', 'against', 'very', 'at', 'of', 'were', \"mightn't\", 'those', 'ourselves', 'hasn', 'off', 'do', 'who', 'or', \"wasn't\", 'after', 'you', 'from', 'won', 'am', 'where'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIxBS7fzoa_p"
      },
      "source": [
        "### Preparing pre-processed datasets and writing to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "nyhbWQSwoa_q",
        "outputId": "0f643b62-2639-4397-a548-9bbf5c97f148"
      },
      "source": [
        "path_to_csv_train = data_dir+'pre-processing/'+'preprocessing_text_train.csv'\n",
        "path_to_csv_test = data_dir+'pre-processing/'+'preprocessing_text_test.csv'\n",
        "path_to_csv_train_without_lemma = data_dir+'preprocessing_text_train_without_lemma.csv'\n",
        "path_to_csv_test_without_lemma = data_dir+'preprocessing_text_test_without_lemma.csv'\n",
        "\n",
        "def write_data_to_csv(path_to_csv, data_frame_name):\n",
        "    data_frame_name.to_csv(path_to_csv, index = False, header=True)\n",
        "\n",
        "# data_train['question1'] = data_train['question1'].apply(pre_processing_without_lemmatization_and_spell_correct)\n",
        "# data_train['question2'] = data_train['question2'].apply(pre_processing_without_lemmatization_and_spell_correct)\n",
        "# data_train = data_train.drop(['Y'], axis=1)\n",
        "# display(data_train)\n",
        "# write_data_to_csv(path_to_csv_train_without_lemma, data_train)\n",
        "\n",
        "data_train['question1'] = data_train['question1'].apply(pre_processing_with_lemmatization)\n",
        "data_train['question2'] = data_train['question2'].apply(pre_processing_with_lemmatization)\n",
        "data_train = data_train.drop(['Y'], axis=1)\n",
        "display(data_train)\n",
        "write_data_to_csv(path_to_csv_train, data_train)\n",
        "\n",
        "# data_test['question1'] = data_test['question1'].apply(pre_processing_without_lemmatization_and_spell_correct)\n",
        "# data_test['question2'] = data_test['question2'].apply(pre_processing_without_lemmatization_and_spell_correct)\n",
        "# data_test = data_test.drop(['Y'], axis=1)\n",
        "# display(data_test)\n",
        "# write_data_to_csv(path_to_csv_test_without_lemma, data_test)\n",
        "\n",
        "data_test['question1'] = data_test['question1'].apply(pre_processing_with_lemmatization)\n",
        "data_test['question2'] = data_test['question2'].apply(pre_processing_with_lemmatization)\n",
        "data_test = data_test.drop(['Y'], axis=1)\n",
        "display(data_test)\n",
        "write_data_to_csv(path_to_csv_test, data_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>394437</td>\n",
              "      <td>434361</td>\n",
              "      <td>527326</td>\n",
              "      <td>install apk file window phone</td>\n",
              "      <td>backup ( xap appx ) file installed window phone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>373988</td>\n",
              "      <td>8023</td>\n",
              "      <td>10567</td>\n",
              "      <td>major effect cambodia earthquake effect compar...</td>\n",
              "      <td>major effect cambodia earthquake effect compar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>183101</td>\n",
              "      <td>280083</td>\n",
              "      <td>280084</td>\n",
              "      <td>stack exchange better quora</td>\n",
              "      <td>stack exchange better quora</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43553</td>\n",
              "      <td>78324</td>\n",
              "      <td>78325</td>\n",
              "      <td>prevent pimple break inside nose</td>\n",
              "      <td>avoid getting pimple inside nose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>213919</td>\n",
              "      <td>319381</td>\n",
              "      <td>46044</td>\n",
              "      <td>good book online course follow grab concept st...</td>\n",
              "      <td>good online course probability statistic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323427</th>\n",
              "      <td>346872</td>\n",
              "      <td>475259</td>\n",
              "      <td>475260</td>\n",
              "      <td>benefit using const declaring constant</td>\n",
              "      <td>benefit using enum declare constant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323428</th>\n",
              "      <td>143678</td>\n",
              "      <td>14804</td>\n",
              "      <td>40458</td>\n",
              "      <td>get complete list old gmail account name</td>\n",
              "      <td>find gmail account list</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323429</th>\n",
              "      <td>128137</td>\n",
              "      <td>14317</td>\n",
              "      <td>34001</td>\n",
              "      <td>found modern colour texture floor tile sydney</td>\n",
              "      <td>get wide range floor tile wall tile porcelain ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323430</th>\n",
              "      <td>323891</td>\n",
              "      <td>449906</td>\n",
              "      <td>449907</td>\n",
              "      <td>support 1877 778 89 69 acer technical support ...</td>\n",
              "      <td>support 1877 778 89 69 compaq technical suppor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323431</th>\n",
              "      <td>228034</td>\n",
              "      <td>337010</td>\n",
              "      <td>337011</td>\n",
              "      <td>lollipop gummy bear manufactured</td>\n",
              "      <td>lollipop gummy bear differ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>323432 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...                                          question2\n",
              "0       394437  ...    backup ( xap appx ) file installed window phone\n",
              "1       373988  ...  major effect cambodia earthquake effect compar...\n",
              "2       183101  ...                        stack exchange better quora\n",
              "3        43553  ...                   avoid getting pimple inside nose\n",
              "4       213919  ...           good online course probability statistic\n",
              "...        ...  ...                                                ...\n",
              "323427  346872  ...                benefit using enum declare constant\n",
              "323428  143678  ...                            find gmail account list\n",
              "323429  128137  ...  get wide range floor tile wall tile porcelain ...\n",
              "323430  323891  ...  support 1877 778 89 69 compaq technical suppor...\n",
              "323431  228034  ...                         lollipop gummy bear differ\n",
              "\n",
              "[323432 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>350255</td>\n",
              "      <td>478982</td>\n",
              "      <td>478983</td>\n",
              "      <td>studying made easy handwritten note bought del...</td>\n",
              "      <td>download different institute handwritten class...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>49376</td>\n",
              "      <td>87885</td>\n",
              "      <td>87886</td>\n",
              "      <td>'t melisandre make shadow baby</td>\n",
              "      <td>would 't stannis use melisandre 's shadow baby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>276580</td>\n",
              "      <td>395562</td>\n",
              "      <td>395563</td>\n",
              "      <td>fix one gear epicyclic gear train power input</td>\n",
              "      <td>antarctica geopolitical importance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13757</td>\n",
              "      <td>26386</td>\n",
              "      <td>26387</td>\n",
              "      <td>sent text friend whatsapp 2 tick soon uninstal...</td>\n",
              "      <td>lately sent whatsapp message friend online mes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>307764</td>\n",
              "      <td>431504</td>\n",
              "      <td>431505</td>\n",
              "      <td>machine learning used borderline pd diagnosis</td>\n",
              "      <td>borderline pd affect writing ability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80853</th>\n",
              "      <td>159203</td>\n",
              "      <td>40624</td>\n",
              "      <td>27380</td>\n",
              "      <td>safety precaution handling shotgun proposed nr...</td>\n",
              "      <td>safety precaution handling shotgun proposed nr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80854</th>\n",
              "      <td>200725</td>\n",
              "      <td>37131</td>\n",
              "      <td>302569</td>\n",
              "      <td>find molar mass deuterium</td>\n",
              "      <td>find molar mass ionic compound</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80855</th>\n",
              "      <td>179551</td>\n",
              "      <td>275483</td>\n",
              "      <td>275484</td>\n",
              "      <td>key witness supported death penalty dzhokhar t...</td>\n",
              "      <td>key witness support death penalty dzhokhar tsa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80856</th>\n",
              "      <td>115590</td>\n",
              "      <td>188458</td>\n",
              "      <td>188459</td>\n",
              "      <td>dam built</td>\n",
              "      <td>dam built</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80857</th>\n",
              "      <td>253135</td>\n",
              "      <td>367605</td>\n",
              "      <td>367606</td>\n",
              "      <td>price varies 2 bhk apartment particular project</td>\n",
              "      <td>chance getting offer german applied science un...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80858 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ...                                          question2\n",
              "0      350255  ...  download different institute handwritten class...\n",
              "1       49376  ...  would 't stannis use melisandre 's shadow baby...\n",
              "2      276580  ...                 antarctica geopolitical importance\n",
              "3       13757  ...  lately sent whatsapp message friend online mes...\n",
              "4      307764  ...               borderline pd affect writing ability\n",
              "...       ...  ...                                                ...\n",
              "80853  159203  ...  safety precaution handling shotgun proposed nr...\n",
              "80854  200725  ...                     find molar mass ionic compound\n",
              "80855  179551  ...  key witness support death penalty dzhokhar tsa...\n",
              "80856  115590  ...                                          dam built\n",
              "80857  253135  ...  chance getting offer german applied science un...\n",
              "\n",
              "[80858 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6MQXuG8oa_r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}