{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_2_Vector",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7pnzggAB7Db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23291134-c489-4686-a72b-d7e78e676515"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"./SG_Drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./SG_Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZJHLvRdHqWS"
      },
      "source": [
        "import os\n",
        "print(os.popen('ls drive/MyDrive/Quora-Data').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us09VU9c7AI5"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttBTuNtXACbe",
        "outputId": "636c6726-0f11-4382-e279-896ca515b46d"
      },
      "source": [
        "!ls SG_Drive/MyDrive/Quora-Data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SG_Drive/MyDrive/Quora-Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj80s2thAHyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c75ff8-0bb1-45d7-a71b-ef0953efd3b7"
      },
      "source": [
        "help(Word2Vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Word2Vec in module gensim.models.word2vec:\n",
            "\n",
            "class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
            " |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
            " |  \n",
            " |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
            " |  \n",
            " |  Once you're finished training a model (=no more updates, only querying)\n",
            " |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
            " |  \n",
            " |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
            " |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
            " |  \n",
            " |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
            " |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
            " |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
            " |  \n",
            " |  Some important attributes are the following:\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
            " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
            " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
            " |  \n",
            " |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
            " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
            " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
            " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
            " |  \n",
            " |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
            " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
            " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
            " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
            " |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Word2Vec\n",
            " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
            " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
            " |      gensim.utils.SaveLoad\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __contains__(self, word)\n",
            " |      Deprecated. Use `self.wv.__contains__` instead.\n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
            " |  \n",
            " |  __getitem__(self, words)\n",
            " |      Deprecated. Use `self.wv.__getitem__` instead.\n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
            " |  \n",
            " |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of iterables, optional\n",
            " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |          See also the `tutorial on data streaming in Python\n",
            " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
            " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
            " |          in some other way.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (or none of them).\n",
            " |      size : int, optional\n",
            " |          Dimensionality of the word vectors.\n",
            " |      window : int, optional\n",
            " |          Maximum distance between the current and predicted word within a sentence.\n",
            " |      min_count : int, optional\n",
            " |          Ignores all words with total frequency lower than this.\n",
            " |      workers : int, optional\n",
            " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
            " |      sg : {0, 1}, optional\n",
            " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
            " |      hs : {0, 1}, optional\n",
            " |          If 1, hierarchical softmax will be used for model training.\n",
            " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
            " |      negative : int, optional\n",
            " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
            " |          should be drawn (usually between 5-20).\n",
            " |          If set to 0, no negative sampling is used.\n",
            " |      ns_exponent : float, optional\n",
            " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
            " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
            " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
            " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupr√©, Lesaint, & Royo-Letelier suggest that\n",
            " |          other values may perform better for recommendation applications.\n",
            " |      cbow_mean : {0, 1}, optional\n",
            " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
            " |      alpha : float, optional\n",
            " |          The initial learning rate.\n",
            " |      min_alpha : float, optional\n",
            " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
            " |      seed : int, optional\n",
            " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
            " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
            " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
            " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
            " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
            " |      max_vocab_size : int, optional\n",
            " |          Limits the RAM during vocabulary building; if there are more unique\n",
            " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
            " |          Set to `None` for no limit.\n",
            " |      max_final_vocab : int, optional\n",
            " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
            " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
            " |          Set to `None` if not required.\n",
            " |      sample : float, optional\n",
            " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
            " |          useful range is (0, 1e-5).\n",
            " |      hashfxn : function, optional\n",
            " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
            " |      iter : int, optional\n",
            " |          Number of iterations (epochs) over the corpus.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
            " |          model.\n",
            " |      \n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |      sorted_vocab : {0, 1}, optional\n",
            " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
            " |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
            " |      batch_words : int, optional\n",
            " |          Target size (in words) for batches of examples passed to worker threads (and\n",
            " |          thus cython routines).(Larger batches will be passed if individual\n",
            " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
            " |      compute_loss: bool, optional\n",
            " |          If True, computes and stores loss value which can be retrieved using\n",
            " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
            " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
            " |          Sequence of callbacks to be executed at specific stages during training.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
            " |      \n",
            " |      >>> from gensim.models import Word2Vec\n",
            " |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |      >>> model = Word2Vec(sentences, min_count=1)\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Human readable representation of the model's state.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      str\n",
            " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
            " |          and learning rate.\n",
            " |  \n",
            " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
            " |      Deprecated. Use `self.wv.accuracy` instead.\n",
            " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
            " |  \n",
            " |  clear_sims(self)\n",
            " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
            " |      \n",
            " |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
            " |  \n",
            " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
            " |      Discard parameters that are used in training and scoring, to save memory.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      Use only if you're sure you're done training a model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      replace_word_vectors_with_normalized : bool, optional\n",
            " |          If True, forget the original (not normalized) word vectors and only keep\n",
            " |          the L2-normalized word vectors, to save even more memory.\n",
            " |  \n",
            " |  get_latest_training_loss(self)\n",
            " |      Get current value of the training loss.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Current training loss.\n",
            " |  \n",
            " |  init_sims(self, replace=False)\n",
            " |      Deprecated. Use `self.wv.init_sims` instead.\n",
            " |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
            " |  \n",
            " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
            " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
            " |      where it intersects with the current vocabulary.\n",
            " |      \n",
            " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
            " |      non-intersecting words are left alone.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          The file path to load the vectors from.\n",
            " |      lockf : float, optional\n",
            " |          Lock-factor value to be set for any imported word-vectors; the\n",
            " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
            " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
            " |      binary : bool, optional\n",
            " |          If True, `fname` is in the binary word2vec C format.\n",
            " |      encoding : str, optional\n",
            " |          Encoding of `text` for `unicode` function (python2 only).\n",
            " |      unicode_errors : str, optional\n",
            " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
            " |  \n",
            " |  predict_output_word(self, context_words_list, topn=10)\n",
            " |      Get the probability distribution of the center word given context words.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      context_words_list : list of str\n",
            " |          List of context words.\n",
            " |      topn : int, optional\n",
            " |          Return `topn` words and their probabilities.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float)\n",
            " |          `topn` length list of tuples of (word, probability).\n",
            " |  \n",
            " |  reset_from(self, other_model)\n",
            " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
            " |      \n",
            " |      Structures copied are:\n",
            " |          * Vocabulary\n",
            " |          * Index to word mapping\n",
            " |          * Cumulative frequency table (used for negative sampling)\n",
            " |          * Cached corpus length\n",
            " |      \n",
            " |      Useful when testing multiple models on the same corpus in parallel.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
            " |          Another model to copy the internal structures from.\n",
            " |  \n",
            " |  save(self, *args, **kwargs)\n",
            " |      Save the model.\n",
            " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
            " |      online training and getting vectors for vocabulary words.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to the file.\n",
            " |  \n",
            " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
            " |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
            " |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
            " |  \n",
            " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
            " |      Score the log probability for a sequence of sentences.\n",
            " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
            " |      \n",
            " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
            " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
            " |      \n",
            " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
            " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
            " |      \n",
            " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
            " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
            " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
            " |      how to use such scores in document classification.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str\n",
            " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |      total_sentences : int, optional\n",
            " |          Count of sentences.\n",
            " |      chunksize : int, optional\n",
            " |          Chunksize of jobs\n",
            " |      queue_factor : int, optional\n",
            " |          Multiplier for size of queue (number of workers * queue_factor).\n",
            " |      report_delay : float, optional\n",
            " |          Seconds to wait before reporting progress.\n",
            " |  \n",
            " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
            " |      Update the model's neural weights from a sequence of sentences.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
            " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
            " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
            " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
            " |      you can simply use `total_examples=self.corpus_count`.\n",
            " |      \n",
            " |      Warnings\n",
            " |      --------\n",
            " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
            " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
            " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str\n",
            " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |          See also the `tutorial on data streaming in Python\n",
            " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (not both of them).\n",
            " |      total_examples : int\n",
            " |          Count of sentences.\n",
            " |      total_words : int\n",
            " |          Count of raw words in sentences.\n",
            " |      epochs : int\n",
            " |          Number of iterations (epochs) over the corpus.\n",
            " |      start_alpha : float, optional\n",
            " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
            " |          for this one call to`train()`.\n",
            " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
            " |          (not recommended).\n",
            " |      end_alpha : float, optional\n",
            " |          Final learning rate. Drops linearly from `start_alpha`.\n",
            " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
            " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
            " |          (not recommended).\n",
            " |      word_count : int, optional\n",
            " |          Count of words already trained. Set this to 0 for the usual\n",
            " |          case of training on all words in sentences.\n",
            " |      queue_factor : int, optional\n",
            " |          Multiplier for size of queue (number of workers * queue_factor).\n",
            " |      report_delay : float, optional\n",
            " |          Seconds to wait before reporting progress.\n",
            " |      compute_loss: bool, optional\n",
            " |          If True, computes and stores loss value which can be retrieved using\n",
            " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
            " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
            " |          Sequence of callbacks to be executed at specific stages during training.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from gensim.models import Word2Vec\n",
            " |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |      >>>\n",
            " |      >>> model = Word2Vec(min_count=1)\n",
            " |      >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
            " |      >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
            " |      (1, 30)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  load(*args, **kwargs) from builtins.type\n",
            " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
            " |          Save model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to the saved file.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
            " |          Loaded model.\n",
            " |  \n",
            " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
            " |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  log_accuracy(section)\n",
            " |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
            " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
            " |  \n",
            " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
            " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str\n",
            " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (not both of them).\n",
            " |      update : bool\n",
            " |          If true, the new words in `sentences` will be added to model's vocab.\n",
            " |      progress_per : int, optional\n",
            " |          Indicates how many words to process before showing/updating the progress.\n",
            " |      keep_raw_vocab : bool, optional\n",
            " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
            " |          of the model.\n",
            " |      \n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |      \n",
            " |      **kwargs : object\n",
            " |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
            " |  \n",
            " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
            " |      Build vocabulary from a dictionary of word frequencies.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      word_freq : dict of (str, int)\n",
            " |          A mapping from a word in the vocabulary to its frequency count.\n",
            " |      keep_raw_vocab : bool, optional\n",
            " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
            " |      corpus_count : int, optional\n",
            " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
            " |          of the model.\n",
            " |      \n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |      \n",
            " |      update : bool, optional\n",
            " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
            " |  \n",
            " |  doesnt_match(self, words)\n",
            " |      Deprecated, use self.wv.doesnt_match() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
            " |  \n",
            " |  estimate_memory(self, vocab_size=None, report=None)\n",
            " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vocab_size : int, optional\n",
            " |          Number of unique tokens in the vocabulary\n",
            " |      report : dict of (str, int), optional\n",
            " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      dict of (str, int)\n",
            " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
            " |  \n",
            " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
            " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
            " |      \n",
            " |      Refer to the documentation for\n",
            " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
            " |  \n",
            " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
            " |      Deprecated, use self.wv.most_similar() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
            " |  \n",
            " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
            " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
            " |      \n",
            " |      Refer to the documentation for\n",
            " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
            " |  \n",
            " |  n_similarity(self, ws1, ws2)\n",
            " |      Deprecated, use self.wv.n_similarity() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
            " |  \n",
            " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
            " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
            " |  \n",
            " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
            " |      Deprecated, use self.wv.similar_by_word() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
            " |  \n",
            " |  similarity(self, w1, w2)\n",
            " |      Deprecated, use self.wv.similarity() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
            " |  \n",
            " |  wmdistance(self, document1, document2)\n",
            " |      Deprecated, use self.wv.wmdistance() instead.\n",
            " |      \n",
            " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
            " |  \n",
            " |  cum_table\n",
            " |  \n",
            " |  hashfxn\n",
            " |  \n",
            " |  iter\n",
            " |  \n",
            " |  layer1_size\n",
            " |  \n",
            " |  min_count\n",
            " |  \n",
            " |  sample\n",
            " |  \n",
            " |  syn0_lockf\n",
            " |  \n",
            " |  syn1\n",
            " |  \n",
            " |  syn1neg\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}